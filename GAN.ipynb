{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Practicum 4 - Generative Adversarial Networks\n",
    "\n",
    "**Topics covered**: Generative Adversarial Networks(GAN) with Multi-layer Perceptron\n",
    "\n",
    "**Deliverables**:\n",
    "- Complete the tasks as detailed in this document.  <br>\n",
    "This practice uses [Keras](https://keras.io/api/) with Tensorflow.\n",
    "\n",
    "**Objectives**:\n",
    "Usually a GAN is referring to DCGAN (deep convolution GAN): a GAN where the generator and discriminator are deep convnets. Training a DCGAN is computational cost. In this practice, deep convnets will be replaced with multi-layer perceptron. <br>\n",
    "In this practice, we're to implement a standard GAN using a multi-layer perceptron for both the discriminator and generator. We train the GAN on MNIST images. After completing the tasks, you'll be get familiaried with the schematic GAN implementation. <br>\n",
    "\n",
    "---\n",
    "Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, optimizers\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Schematically, the GAN we're to implement looks like this: <br>\n",
    "- 1 A generator network maps vectors of shape (latent_dim,) to images of shape (28, 28) which is the size of images from MNIST. <br>\n",
    "- 2 A discriminator network maps images of shape (28, 28) to a binary score estimating the probability that the image is real. <br>\n",
    "- 3 A gan network chains the generator and the discriminator together: gan(x) = discriminator(generator(x)). Thus this gan network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by the generator. <br>\n",
    "- 4 We train the discriminator using examples of real and fake images along with “real”/“fake” labels, just as you train any regular image-classification model. <br>\n",
    "- 5 To train the generator, you use the gradients of the generator’s weights with regard to the loss of the gan model. This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as “real” the images decoded by the generator. In other words, you train the generator to fool the discriminator.\n",
    "\n",
    "The settings of our GAN mode is given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "img_size = 28 * 28   # MNIST images\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data, only need train_data, the task is not for recognition, it doesn't need target\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Data Normalization\n",
    "# Your code goes here\n",
    "\n",
    "\n",
    "# We're using multi-layered model, reshape image matrix into a vector (784,)\n",
    "# Your code goes here\n",
    "\n",
    "\n",
    "# The shape of the normalized and reshaped data set is (60000, 784)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generator\n",
    "First, let’s develop a generator model that turns a vector (from the latent space—during training it will be sampled at random) into a candidate image. <br>\n",
    "- input shape: latent_dim <br>\n",
    "- output shape: img_size <br>\n",
    "Build up a 3-layer feed forward neural network. <br>\n",
    "Suggested structure (you don't have to follow it) is as following: <br>\n",
    "- hidden layer: 128 neuron, `LeakyReLU()`, `Dropout(0.5)`\n",
    "- output layer: `tanh()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discriminator\n",
    "Next, we’ll develop a discriminator model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: “generated image” or “real image that comes from the training set.” <br>\n",
    "Build up a 3-layer feed forward neural network and compile it. <br>\n",
    "Suggested structure (you don't have to follow it) is as follow: <br>\n",
    "- hidden layer: 128 neuron, `LeakyReLU()`, `Dropout(0.5)` <br>\n",
    "- output layer: `sigmod()` (It is compulsory that use sigmod for classification). <br>\n",
    "- optimizer: `RMSprop(lr=0.0001, decay=1e-8)` <br>\n",
    "- loss: `'binary_crossentropy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GAN\n",
    "Finally, we’ll set up the GAN, which chains the generator and the discriminator. <br>\n",
    "When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision—“fake” or “real”—and it’s meant to be trained with labels that are always “these are real images.” So, training gan will update the weights of generator in a way that makes discriminator more likely to predict “real” when looking at fake images. <br>\n",
    "It’s very important to **note** that you set the discriminator to be frozen during training (non-trainable): its weights won’t be updated when training gan. <br>\n",
    "Run the code in cell below to set up the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\n",
    "\n",
    "discriminator.trainable = False                    # set the discriminator to be frozen\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0001, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "Now we can begin training. For each epoch, do the following: <br>\n",
    "- 1 Draw random points in the latent space (random noise). <br>\n",
    "- 2 Generate images with generator using this random noise. <br>\n",
    "- 3 Mix the generated images with real ones. <br>\n",
    "- 4 Train discriminator using these mixed images, with corresponding targets: either “real” (for the real images) or “fake” (for the generated images). <br>\n",
    "- 5 Draw new random points in the latent space. <br>\n",
    "- 6 Train gan using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator (only, because the discriminator is frozen inside gan) to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"D\":[], \"G\":[]}\n",
    "\n",
    "batchCount = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "# After successfully completeing the code in this cell, and hitting the run button\n",
    "# go and grab a cup of tea or coffee, it'll take a while if you're using CPU\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(batchCount):\n",
    "        \n",
    "        ###################################################################\n",
    "        ####  Preparing real images and fake/generated images         #####\n",
    "        ###################################################################\n",
    "    \n",
    "        # Create a batch by drawing random index numbers from the training set x_train         \n",
    "        real_images = # Put your code here\n",
    "    \n",
    "        # Create noise vectors for the generator\n",
    "        # TRICK, use normal distribution not a uniform distribution\n",
    "        random_latent_vectors = # Put your code here\n",
    "        \n",
    "        # Use generator.predict() to generate the images from the noise\n",
    "        generated_images = # put your code here\n",
    "        \n",
    "        # Combine the two types of images\n",
    "        combined_images = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "        # Labels: genteratred - 0; real - 1\n",
    "        labels = # put your code here\n",
    "        # IMPORTANT TRICK: Adds random noise to the labels\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "        \n",
    "        ###################################################################\n",
    "        ####  Train discriminator and generator                       #####\n",
    "        ###################################################################\n",
    "    \n",
    "        # Train the discriminator on the combined_images and labels\n",
    "        d_loss = # put your code here\n",
    "        \n",
    "        \n",
    "        # Train the generator        \n",
    "        # randomly generate the input, use normal distribution\n",
    "        random_latent_vectors = # put your code here\n",
    "        \n",
    "        # Create labels that say “these are all real images”  - it’s a lie! In order to fool the discriminator\n",
    "        misleading_targets = # put your code here\n",
    "        \n",
    "        # Train the generator via the GAN model, where the weights of generator are frozen\n",
    "        g_loss = # put your code here\n",
    "        \n",
    "        # store losses for each batch\n",
    "        losses[\"D\"].append(d_loss)\n",
    "        losses[\"G\"].append(g_loss)\n",
    "        \n",
    "    # plot 10 images from generated samples every 20 epoch\n",
    "    # Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot the learning curve\n",
    "Plot the loss of discriminator and generator respectively. Generator loss should be decreasing along with the iteration number, while discriminator loss is increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "# Your code goes here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
